{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba70859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import hamming_loss, precision_score, f1_score\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "print(f\"Missing values:\\n{data.isnull().sum()}\")\n",
    "\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "text_cols = data.select_dtypes(include=[object]).columns\n",
    "\n",
    "# Handle missing values for numeric columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data[numeric_cols] = imputer.fit_transform(data[numeric_cols])\n",
    "\n",
    "# Handle text columns \n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=500)  \n",
    "text_data = tfidf.fit_transform(data['report'])  \n",
    "\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# Convert the sparse matrix to a DataFrame\n",
    "text_data_df = pd.DataFrame(text_data.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# Combine the processed text data with the rest of the dataset\n",
    "data_imputed = pd.concat([data.drop(columns=text_cols), text_data_df], axis=1)\n",
    "\n",
    "# Check label distribution to see if it's imbalanced\n",
    "label_columns = [col for col in data_imputed.columns if col.startswith('type_')]  \n",
    "label_counts = data_imputed[label_columns].sum()\n",
    "print(f\"Label distribution:\\n{label_counts}\")\n",
    "\n",
    "# Plot the label distribution\n",
    "label_counts.plot(kind='bar', title='Label Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Check if any labels have only one class\n",
    "for col in label_columns:\n",
    "    unique_classes = data_imputed[col].value_counts()\n",
    "    print(f\"Unique classes in label {col}:\\n{unique_classes}\\n\")\n",
    "    \n",
    "    if len(unique_classes) == 1:\n",
    "        print(f\"Warning: Label {col} has only one class: {unique_classes.index[0]}\")\n",
    "\n",
    "# Remove labels with only one class\n",
    "valid_labels = [col for col in label_columns if data_imputed[col].nunique() > 1]\n",
    "print(f\"Valid labels (those with more than one class): {valid_labels}\")\n",
    "\n",
    "# Features and target (after removing labels with only one class)\n",
    "X = data_imputed.drop(columns=label_columns)\n",
    "y = data_imputed[valid_labels]  # Multi-label target with valid labels\n",
    "\n",
    "# Ensure X and y have the same number of rows (this should already be the case)\n",
    "assert X.shape[0] == y.shape[0], \"Mismatch between feature matrix (X) and target matrix (y) dimensions.\"\n",
    "\n",
    "# Feature scaling: Standardize the feature matrix\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (80% train, 10% validation, 10% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split again to create validation and test sets (50% of temp for validation and 50% for test)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning grid for Logistic Regression, SVM, Perceptron, and DNN\n",
    "param_dist_log_reg = {\n",
    "    'estimator__C': uniform(0.01, 100),  # Random values for C\n",
    "    'estimator__solver': ['lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "param_grid_svm = {\n",
    "    'estimator__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'estimator__gamma': ['scale', 'auto'],\n",
    "    'estimator__kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "param_dist_perceptron = {\n",
    "    'estimator__alpha': uniform(0.0001, 0.1),  # Random values for alpha\n",
    "    'estimator__max_iter': [500, 1000, 1500]\n",
    "}\n",
    "\n",
    "param_dist_dnn = {\n",
    "    'estimator__hidden_layer_sizes': [(100,), (200,), (256, 128)],\n",
    "    'estimator__learning_rate_init': uniform(0.001, 0.1),  # Random values for learning rate\n",
    "    'estimator__max_iter': [200, 500, 1000]\n",
    "}\n",
    "\n",
    "# Define models for multi-label classification\n",
    "log_reg_model = MultiOutputClassifier(LogisticRegression(max_iter=5000, class_weight='balanced', solver='saga'))\n",
    "svm_model = MultiOutputClassifier(SVC(probability=True, class_weight='balanced'))\n",
    "svm_model.fit(X_scaled, y)\n",
    "perceptron_model = MultiOutputClassifier(Perceptron(max_iter=5000))\n",
    "dnn_model = MultiOutputClassifier(MLPClassifier(max_iter=2000, learning_rate_init=0.001, early_stopping=True))\n",
    "\n",
    "# Train models only if there are at least two classes for each label\n",
    "if all(y_train[col].sum() > 0 for col in valid_labels):\n",
    "    # RandomizedSearchCV for Logistic Regression\n",
    "    random_search_log_reg = RandomizedSearchCV(estimator=log_reg_model, param_distributions=param_dist_log_reg, n_iter=10, cv=5, n_jobs=-1, verbose=2)\n",
    "    random_search_log_reg.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for Logistic Regression: {random_search_log_reg.best_params_}\")\n",
    "\n",
    "    # RandomizedSearchCV for SVM\n",
    "    grid_search_svm = GridSearchCV(estimator=svm_model, param_grid=param_grid_svm, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search_svm.fit(X_scaled, y)\n",
    "    print(f\"Best parameters for SVM: {grid_search_svm.best_params_}\")\n",
    "\n",
    "    # RandomizedSearchCV for Perceptron\n",
    "    random_search_perceptron = RandomizedSearchCV(estimator=perceptron_model, param_distributions=param_dist_perceptron, n_iter=10, cv=5, n_jobs=-1, verbose=2)\n",
    "    random_search_perceptron.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for Perceptron: {random_search_perceptron.best_params_}\")\n",
    "\n",
    "    # RandomizedSearchCV for DNN\n",
    "    random_search_dnn = RandomizedSearchCV(estimator=dnn_model, param_distributions=param_dist_dnn, n_iter=10, cv=5, n_jobs=-1, verbose=2)\n",
    "    random_search_dnn.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for DNN: {random_search_dnn.best_params_}\")\n",
    "\n",
    "    # Evaluate models using multi-label metrics\n",
    "    def evaluate_tuned(model, X_val, y_val):\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Hamming Loss\n",
    "        hamming = hamming_loss(y_val, y_pred)\n",
    "\n",
    "        # Precision@k\n",
    "        precision_at_k = precision_score(y_val, y_pred, average='micro')\n",
    "\n",
    "        # Micro-F1 and Macro-F1\n",
    "        micro_f1 = f1_score(y_val, y_pred, average='micro')\n",
    "        macro_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "        print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "        print(f\"Micro-F1: {micro_f1:.4f}\")\n",
    "        print(f\"Macro-F1: {macro_f1:.4f}\")\n",
    "        print(f\"Precision@k: {precision_at_k:.4f}\")\n",
    "\n",
    "    # Evaluate all tuned models\n",
    "    print(\"\\nEvaluating Logistic Regression (Tuned)...\")\n",
    "    evaluate_tuned(random_search_log_reg.best_estimator_, X_val, y_val)\n",
    "\n",
    "    print(\"\\nEvaluating SVM (Tuned)...\")\n",
    "    evaluate_tuned(grid_search_svm.best_estimator_, X_val, y_val)\n",
    "\n",
    "    print(\"\\nEvaluating Perceptron (Tuned)...\")\n",
    "    evaluate_tuned(random_search_perceptron.best_estimator_, X_val, y_val)\n",
    "\n",
    "    print(\"\\nEvaluating DNN (Tuned)...\")\n",
    "    evaluate_tuned(random_search_dnn.best_estimator_, X_val, y_val)\n",
    "\n",
    "    # Save the trained models\n",
    "    joblib.dump(random_search_log_reg.best_estimator_, './defect_models/log_reg_model_defect.pkl')\n",
    "    joblib.dump(grid_search_svm.best_estimator_, './defect_models/svm_model_defect.pkl')\n",
    "    joblib.dump(random_search_perceptron.best_estimator_, './defect_models/perceptron_model_defect.pkl')\n",
    "    joblib.dump(random_search_dnn.best_estimator_, './defect_models/dnn_model_defect.h5')\n",
    "\n",
    "else:\n",
    "    print(\"Training aborted due to insufficient classes in the training data.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
